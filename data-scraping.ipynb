{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Data Scraping\n",
    "**Libraries:**\n",
    "- [Pandas](https://pandas.pydata.org/)\n",
    "- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Requests](https://docs.python-requests.org/en/latest/)\n"
   ],
   "id": "721606899e978acf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. [Pandas](https://lms.sdmdigital.id/mod/book/view.php?id=21829&chapterid=366)",
   "id": "c6b1d614c2dafe1c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('data/data.csv', delimiter=';')\n",
    "# Display 5 rows of the DataFrame\n",
    "print(data.head(5))\n",
    "print(data.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. [BeatifulSoup](https://lms.sdmdigital.id/mod/book/view.php?id=21829&chapterid=363)",
   "id": "cb84dbc7b3d5fc2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the website to scrape\n",
    "url = 'https://www.kompas.com/global/read/2025/04/09/123149070/china-akan-larang-semua-film-dari-as-balas-tarif-impor-104-persen-trump'\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Send a GET request to the URL with headers\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the title of the news article\n",
    "    judul_berita = soup.find('h1', class_='read__title').text.strip()\n",
    "\n",
    "    # Print the title of the news article\n",
    "    df_berita = pd.DataFrame({\n",
    "        'judul': [judul_berita]\n",
    "    })\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df_berita)\n",
    "else:\n",
    "    print(f'Error: {response.status_code}, {response.reason}')"
   ],
   "id": "ae43d2cc1734fa65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " **Exercise:** [Simple Web Scraping](https://www.scrapethissite.com/pages/simple/)",
   "id": "33f0e29f4548ea77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "url = 'https://www.scrapethissite.com/pages/simple/'\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "print(f\"URL: {response.url} Status Code: {response.status_code}, Reason: {response.reason}\")\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "country_blocks = soup.find_all('div', class_='col-md-4 country')\n",
    "print(f\"Found {len(country_blocks)} countries.\")\n",
    "\n",
    "# Extract country data\n",
    "countries = []\n",
    "for block in country_blocks:\n",
    "    name = block.find('h3', class_='country-name').text.strip()\n",
    "    capital= block.find('span', class_='country-capital').text.strip()\n",
    "    population = block.find('span', class_='country-population').text.strip()\n",
    "    area = block.find('span', class_='country-area').text.strip()\n",
    "    countries.append({\n",
    "        'name': name,\n",
    "        'capital': capital,\n",
    "        'population': population,\n",
    "        'area': area\n",
    "    })\n",
    "# Convert to DataFrame\n",
    "df_countries = pd.DataFrame(countries)\n",
    "# Display the DataFrame\n",
    "print(df_countries.head(10))\n",
    "\n",
    "# Save to CSV\n",
    "df_countries.to_csv('data/countries.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)\n"
   ],
   "id": "83e7cc1f5f35b271",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " **Exercise:** [Web Scraping Paginate](https://www.scrapethissite.com/pages/forms/)",
   "id": "a604a31d128600d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T05:44:01.149936Z",
     "start_time": "2025-06-16T05:43:50.521478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_url = 'https://www.scrapethissite.com/pages/forms/'\n",
    "response = requests.get(base_url)\n",
    "print(f\"Base URL: {response.url} Status Code: {response.status_code}, Reason: {response.reason}\")\n",
    "\n",
    "teams = []\n",
    "for page in range(1, 7):  # Scrape first 6 pages\n",
    "    page_url = f\"{base_url}?page_num={page}&per_page=100\"\n",
    "    response = requests.get(page_url)\n",
    "    print(f\"Page {page} URL: {response.url} Status Code: {response.status_code}, Reason: {response.reason}\")\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Find the table\n",
    "    table = soup.find('table', class_='table')\n",
    "    rows = table.find_all('tr', class_='team')\n",
    "\n",
    "    # Extract team data\n",
    "    for row in rows:\n",
    "        name = row.find('td', class_='name').text.strip()\n",
    "        year = row.find('td', class_='year').text.strip()\n",
    "        wins = row.find('td', class_='wins').text.strip()\n",
    "        losses = row.find('td', class_='losses').text.strip()\n",
    "        ot_losses = row.find('td', class_='ot-losses').text.strip()\n",
    "        pct = row.find('td', class_='pct').text.strip()\n",
    "        gf = row.find('td', class_='gf').text.strip()\n",
    "        ga = row.find('td', class_='ga').text.strip()\n",
    "        diff = row.find('td', class_='diff').text.strip()\n",
    "\n",
    "        teams.append({\n",
    "            'Team Name': name,\n",
    "            'Year': year,\n",
    "            'Wins': wins,\n",
    "            'Losses': losses,\n",
    "            'OT Losses': ot_losses,\n",
    "            'Win %': pct,\n",
    "            'Goal For (GF)': gf,\n",
    "            'Goal Against (GA)': ga,\n",
    "            '+ / -': diff\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_teams = pd.DataFrame(teams)\n",
    "# Display the DataFrame\n",
    "print(f\"Title: {soup.title.string}\")\n",
    "print(f\"Found {len(df_teams)} teams.\")\n",
    "print(df_teams.head(10))\n",
    "\n",
    "# Save to CSV\n",
    "df_teams.to_csv('data/teams.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "print(\"Data saved to data/teams.csv\")"
   ],
   "id": "208ed3444e68238c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base URL: https://www.scrapethissite.com/pages/forms/ Status Code: 200, Reason: OK\n",
      "Page 1 URL: https://www.scrapethissite.com/pages/forms/?page_num=1&per_page=100 Status Code: 200, Reason: OK\n",
      "Page 2 URL: https://www.scrapethissite.com/pages/forms/?page_num=2&per_page=100 Status Code: 200, Reason: OK\n",
      "Page 3 URL: https://www.scrapethissite.com/pages/forms/?page_num=3&per_page=100 Status Code: 200, Reason: OK\n",
      "Page 4 URL: https://www.scrapethissite.com/pages/forms/?page_num=4&per_page=100 Status Code: 200, Reason: OK\n",
      "Page 5 URL: https://www.scrapethissite.com/pages/forms/?page_num=5&per_page=100 Status Code: 200, Reason: OK\n",
      "Page 6 URL: https://www.scrapethissite.com/pages/forms/?page_num=6&per_page=100 Status Code: 200, Reason: OK\n",
      "Title: Hockey Teams: Forms, Searching and Pagination | Scrape This Site | A public sandbox for learning web scraping\n",
      "Found 582 teams.\n",
      "               Team Name  Year Wins Losses OT Losses  Win % Goal For (GF)  \\\n",
      "0          Boston Bruins  1990   44     24             0.55           299   \n",
      "1         Buffalo Sabres  1990   31     30            0.388           292   \n",
      "2         Calgary Flames  1990   46     26            0.575           344   \n",
      "3     Chicago Blackhawks  1990   49     23            0.613           284   \n",
      "4      Detroit Red Wings  1990   34     38            0.425           273   \n",
      "5        Edmonton Oilers  1990   37     37            0.463           272   \n",
      "6       Hartford Whalers  1990   31     38            0.388           238   \n",
      "7      Los Angeles Kings  1990   46     24            0.575           340   \n",
      "8  Minnesota North Stars  1990   27     39            0.338           256   \n",
      "9     Montreal Canadiens  1990   39     30            0.487           273   \n",
      "\n",
      "  Goal Against (GA) + / -  \n",
      "0               264    35  \n",
      "1               278    14  \n",
      "2               263    81  \n",
      "3               211    73  \n",
      "4               298   -25  \n",
      "5               272     0  \n",
      "6               276   -38  \n",
      "7               254    86  \n",
      "8               266   -10  \n",
      "9               249    24  \n",
      "Data saved to data/teams.csv\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " **Exercise:** [Web Scraping with API](https://www.scrapethissite.com/pages/api/)",
   "id": "6c9ff0ab32f2f05f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
